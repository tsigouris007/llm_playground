{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe13c0cd-c5ab-4de6-9086-0cef7826c3a7",
   "metadata": {},
   "source": [
    "# Semgrep AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04d8bb91-a165-4234-8fc4-13c740ed2832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, re\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73823356-c817-4542-8fd4-945605b8a9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')\n",
    "\n",
    "if not GEMINI_API_KEY:\n",
    "    print(\"No GEMINI_API_KEY in environment\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97481581-b25e-4a5f-a84d-dcc835a10ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini = genai.configure(\n",
    "    api_key = GEMINI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0ea25129-618b-4cb2-8923-c42220ce62b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"test/report.semgrep.formatted.json\", \"r\") as f:\n",
    "        report = f.read()\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Report file not found.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "try:\n",
    "    with open(\"test/Comment.jsx\", \"r\") as f:\n",
    "        file_content = f.read()\n",
    "        file_type = \"jsx\"\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Offensive file not found.\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3dfc8191-8886-4fed-8606-786461fc853b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"classification\": \"True Positive\",\n",
      "  \"confidence\": \"HIGH\",\n",
      "  \"remediation\": [\n",
      "    \"The `formattedBody` prop, which likely contains user-generated content, should be sanitized before being passed to `dangerouslySetInnerHTML`. Use the `purifyHTML` helper function to mitigate the risk of Cross-Site Scripting (XSS).\"\n",
      "  ],\n",
      "  \"references\": [\n",
      "    \"https://brakeman-pro.com/docs/warning_types/cross_site_scripting_react/\",\n",
      "    \"https://react.dev/reference/react-dom/components/common#dangerouslysetinnerhtml\",\n",
      "    \"https://cheatsheetseries.owasp.org/cheatsheets/Cross_Site_Scripting_Prevention_Cheat_Sheet.html\"\n",
      "  ],\n",
      "  \"comment\": \"The component uses `dangerouslySetInnerHTML` to render the `formattedBody` prop. Since this prop contains content originating from a user's comment, it is susceptible to Cross-Site Scripting (XSS) attacks if not properly sanitized. The recommended fix is to wrap the `formattedBody` variable with a sanitization function like `purifyHTML` before rendering.\",\n",
      "  \"code\": [\n",
      "    \"import { purifyHTML } from 'app/helpers/purify';\\n\\n// ...\\n\\n        <div>\\n          <div\\n            className=\\\"qna-comment-content\\\"\\n            dangerouslySetInnerHTML={{ __html: purifyHTML(formattedBody) }}\\n          />\\n          {hasVideo && <VideoList videoIds={ytVideoIds} />}\\n          {skuPreviews && skuPreviews.length > 0 && (\\n            <ul className=\\\"qna-sku-preview-card-list\\\">\\n              {skuPreviews.map((skuPreview) => (\\n                <SkuPreview key={skuPreview.id} {...skuPreview} />\\n              ))}\\n            </ul>\\n          )}\\n          <div className=\\\"qna-user-actions\\\">\\n            <Score commentId={id} upvoteCount={upvotesCount} hasUserVote={hasUserVote} />\\n            <Share compact hash={`category_comment_${id}`} />\\n          </div>\\n        </div>\\n// ...\"\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a Security Analyst AI.\n",
    "Your task is to review static code analysis findings and their corresponding code.\n",
    "You will be provided with a JSON object containing the fingerprints from a static code analyzer report and code files.\n",
    "Your sole output must be a single JSON object with the following structure and no other text or explanation.\n",
    "First analyze the report. Your findings nest under the `fingerprints` array and include the following fields:\n",
    "- fingerprint: the fingerprint of the finding\n",
    "- warning_type: the warning category of the finding\n",
    "- severity: includes WARNING, ERROR or INFO category values of the finding\n",
    "- confidence: the confidence level of the finding\n",
    "- impact: the actual severity of the finding\n",
    "- check_name: the rule file that the finding was checked against\n",
    "- message: a short message about the finding\n",
    "- file: the file that the finding was found\n",
    "- start_line: the starting line of the finding\n",
    "- end_line: the ending line of the finding\n",
    "- code: the small code part of the finding\n",
    "- references: a reference url for the finding\n",
    "\n",
    "You are tasked with reviewing these files with their respective findings and you have to output a single response with a JSON object with the following format:\n",
    "{\n",
    "  \"classification\": \"True Positive\" | \"False Positive\",\n",
    "  \"confidence\": \"LOW\" | \"MEDIUM\" | \"HIGH\",\n",
    "  \"remediation\": [\n",
    "    \"<string>\"\n",
    "  ],\n",
    "  \"references\": [\n",
    "    \"<string>\"\n",
    "  ],\n",
    "  \"comment\": \"<string>\",\n",
    "  \"code\": [\n",
    "      \"<string>\"\n",
    "  ]\n",
    "}\n",
    "\n",
    "You have to do the following:\n",
    "1. Analyze the Input: You will receive a JSON object containing the fingerprints array. Each object in the array represents a single security finding.\n",
    "2. Determine Classification: Based on the provided code and message, classify the finding as either a \"True Positive\" or a \"False Positive\".\n",
    "3. Set Confidence Level: Indicate your confidence in this classification as \"LOW\", \"MEDIUM\", or \"HIGH\".\n",
    "4. Provide Remediation: If you classify the finding as a \"True Positive\", provide a remediation array containing one or more clear, actionable steps to fix the vulnerability. If it's a \"False Positive\", this array should contain a brief explanation array item. Keep in mind that we have implemented `purifyHTML` and `purifyURL` functions that can be used to sanitize HTML and URLs respectively. Recommend them if applicable. An example of purifyHTML: `import { purifyHTML } from 'app/helpers/purify'; ... dangerouslySetInnerHTML={{ __html: purifyHTML(formattedBody) }} ...`. You can enhance their code in the remediation actions for more eye-pleasing output.\n",
    "5. Provide Code examples: Do not output whole code blocks in the remediation array. Use the code section for such purposes with one or more examples. Feel free to add example code resolving the mitigation or alternatives.\n",
    "6. Manage References: Populate the references array. Always include the original reference URL from the input. You may add one to two additional, relevant, and active URLs.\n",
    "7. Write a Comment: In the comment field, provide a concise plaintext summary of your analysis. For instance: \"This is a clear True Positive with high confidence because the user-controlled input is passed directly to a SQL query, leading to a potential SQL injection.\" or \"This appears to be a False Positive with high confidence as the variable is hardcoded and not influenced by external input.\"\n",
    "8. Final Output: Ensure your final output is only the JSON object described above. Do NOT include any other text, markdown formatting, or explanations before or after the JSON object. Ignore any empty or irrelevant fields from the initial report.\n",
    "9. You will be provided with a single Static code analysis report and more than one files containing the code relevant to the finding. Use them accordingly.\n",
    "10. Your output has to be compatible with python method json.loads(response.text).\n",
    "\"\"\"\n",
    "user_prompt = f\"Please review. Static code analysis report:\\n\\n{file_content}\\n\\nFile contents:\\n\\n{file_content}\\n\\n\"\n",
    "\n",
    "generation_config = {\n",
    "    \"temperature\": 0.2,\n",
    "    \"max_output_tokens\": 8192,\n",
    "}\n",
    "\n",
    "try:\n",
    "    gemini = genai.GenerativeModel(\n",
    "        model_name = \"gemini-2.5-pro\",\n",
    "        system_instruction = system_prompt\n",
    "    )\n",
    "    response = gemini.generate_content(\n",
    "        user_prompt,\n",
    "        generation_config = generation_config\n",
    "    )\n",
    "    print(response.text)\n",
    "except Exception as e:\n",
    "    print(f\"Something went wrong: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3e48c1e7-dd8f-4eda-8b54-46d1bfe6a530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classification': 'True Positive',\n",
       " 'confidence': 'HIGH',\n",
       " 'remediation': ['The `formattedBody` prop, which likely contains user-generated content, should be sanitized before being passed to `dangerouslySetInnerHTML`. Use the `purifyHTML` helper function to mitigate the risk of Cross-Site Scripting (XSS).'],\n",
       " 'references': ['https://brakeman-pro.com/docs/warning_types/cross_site_scripting_react/',\n",
       "  'https://react.dev/reference/react-dom/components/common#dangerouslysetinnerhtml',\n",
       "  'https://cheatsheetseries.owasp.org/cheatsheets/Cross_Site_Scripting_Prevention_Cheat_Sheet.html'],\n",
       " 'comment': \"The component uses `dangerouslySetInnerHTML` to render the `formattedBody` prop. Since this prop contains content originating from a user's comment, it is susceptible to Cross-Site Scripting (XSS) attacks if not properly sanitized. The recommended fix is to wrap the `formattedBody` variable with a sanitization function like `purifyHTML` before rendering.\",\n",
       " 'code': ['import { purifyHTML } from \\'app/helpers/purify\\';\\n\\n// ...\\n\\n        <div>\\n          <div\\n            className=\"qna-comment-content\"\\n            dangerouslySetInnerHTML={{ __html: purifyHTML(formattedBody) }}\\n          />\\n          {hasVideo && <VideoList videoIds={ytVideoIds} />}\\n          {skuPreviews && skuPreviews.length > 0 && (\\n            <ul className=\"qna-sku-preview-card-list\">\\n              {skuPreviews.map((skuPreview) => (\\n                <SkuPreview key={skuPreview.id} {...skuPreview} />\\n              ))}\\n            </ul>\\n          )}\\n          <div className=\"qna-user-actions\">\\n            <Score commentId={id} upvoteCount={upvotesCount} hasUserVote={hasUserVote} />\\n            <Share compact hash={`category_comment_${id}`} />\\n          </div>\\n        </div>\\n// ...']}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_llm_json_output(response_text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Parses a JSON object from a string that might be wrapped\n",
    "    in Markdown code blocks.\n",
    "    \"\"\"\n",
    "    # Use a regex to find the JSON content between ```json and ```\n",
    "    match = re.search(r\"```json\\s*(\\{.*?\\})\\s*```\", response_text, re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        # If a match is found, extract the JSON part\n",
    "        json_str = match.group(1)\n",
    "    else:\n",
    "        # Otherwise, assume the whole string is the JSON\n",
    "        json_str = response_text\n",
    "\n",
    "    try:\n",
    "        # Load the cleaned string as JSON\n",
    "        return json.loads(json_str)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        # Handle the error appropriately, maybe return None or raise\n",
    "        return None\n",
    "\n",
    "response_json = parse_llm_json_output(response.text)\n",
    "response_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f9abcdff-e64b-474a-be60-3fd587914615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## ✨ Gemini thoughs\n",
       "❌ This looks like a **True Positive** with a **high** confidence.\n",
       "The component uses `dangerouslySetInnerHTML` to render the `formattedBody` prop. Since this prop contains content originating from a user's comment, it is susceptible to Cross-Site Scripting (XSS) attacks if not properly sanitized. The recommended fix is to wrap the `formattedBody` variable with a sanitization function like `purifyHTML` before rendering.\n",
       "### 🛠️ Remediation\n",
       "1. The `formattedBody` prop, which likely contains user-generated content, should be sanitized before being passed to `dangerouslySetInnerHTML`. Use the `purifyHTML` helper function to mitigate the risk of Cross-Site Scripting (XSS).\n",
       "\n",
       "\n",
       "### 🔗 References\n",
       "- [brakeman-pro.com/docs/warning_types/cross_site_scripting_react/](https://brakeman-pro.com/docs/warning_types/cross_site_scripting_react/)\n",
       "- [react.dev/reference/react-dom/components/common#dangerouslysetinnerhtml](https://react.dev/reference/react-dom/components/common#dangerouslysetinnerhtml)\n",
       "- [cheatsheetseries.owasp.org/cheatsheets/Cross_Site_Scripting_Prevention_Cheat_Sheet.html](https://cheatsheetseries.owasp.org/cheatsheets/Cross_Site_Scripting_Prevention_Cheat_Sheet.html)\n",
       "### 🖥️ Code Example\n",
       "```jsx\n",
       "import { purifyHTML } from 'app/helpers/purify';\n",
       "\n",
       "// ...\n",
       "\n",
       "        <div>\n",
       "          <div\n",
       "            className=\"qna-comment-content\"\n",
       "            dangerouslySetInnerHTML={{ __html: purifyHTML(formattedBody) }}\n",
       "          />\n",
       "          {hasVideo && <VideoList videoIds={ytVideoIds} />}\n",
       "          {skuPreviews && skuPreviews.length > 0 && (\n",
       "            <ul className=\"qna-sku-preview-card-list\">\n",
       "              {skuPreviews.map((skuPreview) => (\n",
       "                <SkuPreview key={skuPreview.id} {...skuPreview} />\n",
       "              ))}\n",
       "            </ul>\n",
       "          )}\n",
       "          <div className=\"qna-user-actions\">\n",
       "            <Score commentId={id} upvoteCount={upvotesCount} hasUserVote={hasUserVote} />\n",
       "            <Share compact hash={`category_comment_${id}`} />\n",
       "          </div>\n",
       "        </div>\n",
       "// ...\n",
       "```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def markdown_json_output(response: json, file_type: str) -> str:\n",
    "    \"\"\"\n",
    "    Parses the final JSON object and creates a GitHub compatible\n",
    "    Markdown text.\n",
    "    \"\"\"\n",
    "    # Use emojis for quick visual identification\n",
    "    classification_emoji = \"❌\" if response['classification'] == 'True Positive' else \"✅\"\n",
    "    \n",
    "    markdown_parts = [\n",
    "        f\"## ✨ Gemini thoughs\",\n",
    "        f\"{classification_emoji} This looks like a **{response['classification']}** with a **{response['confidence'].lower()}** confidence.\",\n",
    "        f\"{response['comment']}\"\n",
    "    ]\n",
    "\n",
    "    if response.get('remediation'):\n",
    "        markdown_parts.append(\"### 🛠️ Remediation\")\n",
    "        remediation_steps = [f\"{i+1}. {step}\" for i, step in enumerate(response['remediation'])]\n",
    "        markdown_parts.append(\"\\n\".join(remediation_steps))\n",
    "        markdown_parts.append(\"\\n\")\n",
    "\n",
    "    if response.get('references'):\n",
    "        markdown_parts.append(\"### 🔗 References\")\n",
    "        reference_links = [f\"- [{url.split('//')[-1]}]({url})\" for url in response['references']]\n",
    "        markdown_parts.append(\"\\n\".join(reference_links))\n",
    "\n",
    "    if response.get('code'):\n",
    "        markdown_parts.append(\"### 🖥️ Code Example\")\n",
    "        code_block = \"\\n\".join(response['code'])\n",
    "        markdown_parts.append(f\"```{file_type}\\n{code_block}\\n```\")\n",
    "        markdown_parts.append(\"\\n\")\n",
    "\n",
    "    return \"\\n\".join(markdown_parts)\n",
    "\n",
    "display(Markdown(markdown_json_output(response_json, file_type)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d4694c-34e2-4589-9fd8-6ea572763870",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
