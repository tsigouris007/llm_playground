{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81fa3b43-b66f-42f5-9119-7759032a0f31",
   "metadata": {},
   "source": [
    "# Gradio\n",
    "\n",
    "Learn more: https://www.gradio.app/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2653b4-8a73-4850-b385-5ea7c6fdba6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import google.generativeai\n",
    "import anthropic\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6af6990-d817-41bb-b57d-5e3ad83c0d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY')\n",
    "GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"OPENAI_API_KEY not loaded.\")\n",
    "    sys.exit(1)\n",
    "if not ANTHROPIC_API_KEY:\n",
    "    print(\"ANTHROPIC_API_KEY not loaded.\")\n",
    "    sys.exit(1)\n",
    "if not GEMINI_API_KEY:\n",
    "    print(\"GEMINI_API_KEY not loaded.\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf080d56-29e8-4333-9432-7d8b894a08ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()\n",
    "claude = anthropic.Anthropic()\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea87e9f-3f71-46a0-afaa-7ae9c62708d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_gpt(prompt):\n",
    "    completion = openai.chat.completions.create(\n",
    "        model = \"gpt-4o-mini\",\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebcaa24-4899-49af-9021-d72d767df7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_gpt(\"Who is currently the president of USA?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90059e34-aaa3-4fa2-95ad-4804ce1cb60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shout(text: str):\n",
    "    return text.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee54e4ad-ac61-40c3-a24d-b96d81d325c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "view = gr.Interface(\n",
    "    fn = shout,\n",
    "    inputs = [\n",
    "        gr.Textbox(label = \"Your message\", placeholder = \"Your message\", lines = 6)\n",
    "    ],\n",
    "    outputs = [\n",
    "        gr.Textbox(label = \"Response\", placeholder = \"Response\", lines = 8)\n",
    "    ],\n",
    "    flagging_mode = \"never\",\n",
    "    css = \"footer{display:none !important}\"\n",
    ")\n",
    "# view.launch(share=True) # To create a publicly available URL for gradio\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01563ef-68a0-496d-9c39-2cbee95a373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "view = gr.Interface(\n",
    "    fn = message_gpt,\n",
    "    inputs = [\n",
    "        gr.Textbox(label = \"Your message\", placeholder = \"Your message\", lines = 6)\n",
    "    ],\n",
    "    outputs = [\n",
    "        gr.Textbox(label = \"Response\", placeholder = \"Response\", lines = 8)\n",
    "    ],\n",
    "    flagging_mode = \"never\",\n",
    "    css = \"footer{display: none !important}\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b7f6c7-1f1c-4b74-a6a9-65a73c3c0af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "view = gr.Interface(\n",
    "    fn = message_gpt,\n",
    "    inputs = [\n",
    "        gr.Textbox(label = \"Your message\", placeholder = \"Your message\", lines = 6)\n",
    "    ],\n",
    "    outputs = [\n",
    "        gr.Markdown(label = \"Reponse\")\n",
    "    ],\n",
    "    flagging_mode = \"never\",\n",
    "    css = \"footer{display: none !important}\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534bd200-930d-4308-8bac-b820d3eca13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_gpt(prompt):\n",
    "    stream = openai.chat.completions.create(\n",
    "        model = \"gpt-4o-mini\",\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        stream = True\n",
    "    )\n",
    "    result = \"\"\n",
    "    for chunk in stream:\n",
    "        result += chunk.choices[0].delta.content or \"\"\n",
    "        yield result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ee98d5-e328-4a76-b742-3e1c328fc24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "view = gr.Interface(\n",
    "    fn = stream_gpt,\n",
    "    inputs = [\n",
    "        gr.Textbox(label = \"Your message\", placeholder = \"Your message\", lines = 6)\n",
    "    ],\n",
    "    outputs = [\n",
    "        gr.Markdown(label = \"Reponse\")\n",
    "    ],\n",
    "    flagging_mode = \"never\",\n",
    "    css = \"footer{display: none !important}\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dcb319-5601-4caa-bd25-7d9176e4f3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_claude(prompt):\n",
    "    result = claude.messages.stream(\n",
    "        model = \"claude-3-haiku-20240307\",\n",
    "        max_tokens = 500,\n",
    "        temperature = 0.7,\n",
    "        system = \"You are a helpful assistant\",\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    response = \"\"\n",
    "    with result as stream:\n",
    "        for text in stream.text_stream:\n",
    "            response += text or \"\"\n",
    "            yield response\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8553fed0-3222-4f99-9d1b-bc7603d45b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "view = gr.Interface(\n",
    "    fn = stream_claude,\n",
    "    inputs = [\n",
    "        gr.Textbox(label = \"Your message\", placeholder = \"Your message\", lines = 6)\n",
    "    ],\n",
    "    outputs = [\n",
    "        gr.Markdown(label = \"Reponse\")\n",
    "    ],\n",
    "    flagging_mode = \"never\",\n",
    "    css = \"footer{display: none !important}\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4a8008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_gemini(prompt):\n",
    "    model = google.generativeai.GenerativeModel(\"gemini-2.5-flash\", system_instruction = \"You are a helpful assistant\")\n",
    "    response_stream = model.generate_content(\n",
    "        prompt,\n",
    "        stream = True,\n",
    "        generation_config = google.generativeai.GenerationConfig(\n",
    "            temperature = 0.7,\n",
    "            max_output_tokens = 500\n",
    "        )\n",
    "    )\n",
    "    response = \"\"\n",
    "    for chunk in response_stream:\n",
    "        response += chunk.text or \"\"\n",
    "        yield response\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffbb8e5-20b4-42cb-b301-b6a3bab0b9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_model(prompt, model):\n",
    "    if model.startswith(\"GPT\"):\n",
    "        result = stream_gpt(prompt)\n",
    "    elif model.startswith(\"Claude\"):\n",
    "        result = stream_claude(prompt)\n",
    "    elif model.startswith(\"Gemini\"):\n",
    "        result = stream_gemini(prompt)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model type\")\n",
    "    for chunk in result:\n",
    "        yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d0deab",
   "metadata": {},
   "outputs": [],
   "source": [
    "view = gr.Interface(\n",
    "    fn = stream_model,\n",
    "    inputs = [\n",
    "        gr.Textbox(label = \"Your message\", placeholder = \"Your message\", lines = 6),\n",
    "        gr.Dropdown([\"GPT-4\", \"Claude\", \"Gemini\"], label=\"Select Model\", value=\"GPT-4\")\n",
    "    ],\n",
    "    outputs = [\n",
    "        gr.Markdown(label = \"Reponse\")\n",
    "    ],\n",
    "    flagging_mode = \"never\",\n",
    "    css = \"footer{display: none !important}\"\n",
    ")\n",
    "view.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
