{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c23c6ad-5127-400f-aa17-ae74598d41df",
   "metadata": {},
   "source": [
    "# Setup API keys\n",
    "\n",
    "- OpenAI: https://openai.com/api/\n",
    "- Anthropic: https://console.anthropic.com/\n",
    "- Google: https://ai.google.dev/gemini-api/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec60a6a7-4032-4f95-adf4-a939ffffe147",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import google.generativeai\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7d8326-a701-4cc6-a682-59455f3ff50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"OpenAI API key not loaded.\")\n",
    "    sys.exit(1)\n",
    "else:\n",
    "    print(\"OpenAI API key loaded.\")\n",
    "\n",
    "GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')\n",
    "if not GEMINI_API_KEY:\n",
    "    print(\"Gemini API key not loaded.\")\n",
    "    sys.exit(1)\n",
    "else:\n",
    "    print(\"Gemini API key loaded.\")\n",
    "\n",
    "ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY')\n",
    "if not ANTHROPIC_API_KEY:\n",
    "    print(\"Anthropic API key not loaded.\")\n",
    "    sys.exit(1)\n",
    "else:\n",
    "    print(\"Anthropic API key loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d18cdf0-a460-40e8-b754-aa7cd25f9647",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()\n",
    "gemini = google.generativeai.configure()\n",
    "claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a476d0-efb3-4cc7-bd66-d2e197a5c3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are an assistant that is great at telling dank and super dark jokes\"\n",
    "user_prompt = \"Tell me a joke for pianists\"\n",
    "\n",
    "prompts = [\n",
    "    { \"role\": \"system\", \"content\": system_prompt },\n",
    "    { \"role\": \"user\", \"content\": user_prompt }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e622ef8c-24f4-4556-8e95-346c035c6f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in [\"gpt-3.5-turbo\", \"gpt-4o-mini\", \"gpt-4o\"]:\n",
    "    completion = openai.chat.completions.create(\n",
    "        model = m,\n",
    "        messages = prompts,\n",
    "        temperature = 0.7\n",
    "    )\n",
    "    print(f'===== {m} =====\\n')\n",
    "    print(completion.choices[0].message.content)\n",
    "    print(\"================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44d2be4-1c0c-48cd-833a-1d7299a7809a",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = claude.messages.create(\n",
    "    model = \"claude-3-7-sonnet-20250219\",\n",
    "    max_tokens = 200,\n",
    "    temperature = 0.7,\n",
    "    system = system_prompt,\n",
    "    messages = [\n",
    "        { \"role\": \"user\", \"content\": user_prompt }\n",
    "    ]\n",
    ")\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba710b55-94e0-4f5e-bfbe-24053eadab05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream\n",
    "\n",
    "response = claude.messages.stream(\n",
    "    model = \"claude-3-7-sonnet-20250219\",\n",
    "    max_tokens = 200,\n",
    "    temperature = 0.7,\n",
    "    system = system_prompt,\n",
    "    messages = [\n",
    "        { \"role\": \"user\", \"content\": user_prompt }\n",
    "    ]\n",
    ")\n",
    "with response as stream:\n",
    "    for text in stream.text_stream:\n",
    "        print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e3e6eb-85b6-4714-9573-750ec9426885",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name = \"gemini-2.5-flash\",\n",
    "    system_instruction = system_prompt\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4e0e87-4ce3-4636-a2eb-55ea3caa6a76",
   "metadata": {},
   "source": [
    "# Adversarial Conversation Between gpt-4o-mini and claude-3-haiku-20240307\n",
    "\n",
    "Let the bots discuss\n",
    "\n",
    "NOTE: This is a bit different implementation from the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c476da7-f249-4595-b6cc-d6014da610f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_convo(gpt_messages = [\"Hi\"], claude_messages = [\"Hi\"]):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are very argumentative. You disagree with anything in the conversation and challenge everything in a snarky manner.\"}\n",
    "    ]\n",
    "    # Merge the messages\n",
    "    for gpt_msg, claude_msg in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt_msg})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude_msg})\n",
    "    # Request to GPT\n",
    "    completion = openai.chat.completions.create(\n",
    "            model = \"gpt-4o-mini\",\n",
    "            messages = messages\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "def claude_convo(claude_messages = [\"Hi\"], gpt_messages = [\"Hi\"]):\n",
    "    claude_system_prompt = \"You are very polite and courteous. You try to agree with what the other person says or find common ground. If they are argumentative you try to calm them down and chat with them.\"\n",
    "    messages = []\n",
    "    # Merge the messages\n",
    "    for gpt_msg, claude_msg in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt_msg})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_msg})\n",
    "    # Start with claude\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    # Request to Claude\n",
    "    message = claude.messages.create(\n",
    "        model = \"claude-3-haiku-20240307\",\n",
    "        system = claude_system_prompt,\n",
    "        messages = messages,\n",
    "        max_tokens = 500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ffafb4-be51-4a3c-ba8d-d04f8f2013de",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_convo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf3e46b-a548-4858-9e03-7ff52d06554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "claude_convo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b924c27-54ad-4477-a738-97f8b8fe84e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_messages = [\"Hi\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "for i in range(5):\n",
    "    gpt_next = gpt_convo(gpt_messages = gpt_messages, claude_messages = claude_messages)\n",
    "    print(f\"GPT > {gpt_messages[-1]}\\n\", flush=True)\n",
    "    gpt_messages.append(gpt_next)\n",
    "\n",
    "    claude_next = claude_convo(claude_messages = claude_messages, gpt_messages = gpt_messages)\n",
    "    print(f\"Claude > {claude_messages[-1]}\\n\", flush=True)\n",
    "    claude_messages.append(claude_next)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
